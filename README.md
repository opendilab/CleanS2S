# CleanS2S

English | [ç®€ä½“ä¸­æ–‡(Simplified Chinese)](https://github.com/opendilab/CleanS2S/blob/main/README.zh.md) 

**CleanS2S** is a Speech-to-Speech (**S2S**) prototype agent that provides high-quality and streaming interactions in the single-file implementation. This design is simple and clean, aiming to provide a 
Chinese interactive prototype agent like the GPT-4o style. This project wants to let users directly experience the power of Linguistic User Interface (**LUI**) and quickly explore/vailidate the potential of the S2S pipeline for researchers.

Here are some live conversation demos of CleanS2S:

> Note: please unmute the video first.


<table>
<tr>
<td align="center">

**æŠ•èµ„è¯é¢˜1**

</td>
<td align="center">

**æŠ•èµ„è¯é¢˜2**

</td>
<td align="center">

**å¿ƒæƒ…è¯é¢˜**

</td>
<td align="center">

**é«˜è€ƒå¿—æ„¿è¯é¢˜**

</td>
</tr>
<tr>
<td align="center">

[æŠ•èµ„è¯é¢˜1](https://github.com/user-attachments/assets/65333528-b07c-42ab-9cb5-660b68b404c4)

</td>
<td align="center">

[æŠ•èµ„è¯é¢˜2](https://github.com/user-attachments/assets/f6ee3bad-ddd0-404f-9995-088ac1902b11)

</td>
<td align="center">

[å¿ƒæƒ…è¯é¢˜](https://github.com/user-attachments/assets/40d20126-9c6b-45db-8ee9-ce768fee5b3f)

</td>
<td align="center">

[é«˜è€ƒå¿—æ„¿è¯é¢˜](https://github.com/user-attachments/assets/e86c1cad-ca49-4145-8c22-8d9de59f44b4)

</td>
</tr>
</table>


<br>
<details>
<summary><strong style="font-size: 1.5em;">More Conversation Demos</strong></summary>
<br>

<table>
<tr>
<td align="center">

**èƒƒç—…è¯é¢˜**

</td>
</tr>
<tr>
<td align="center">

[èƒƒè¯è¯é¢˜](https://github.com/user-attachments/assets/84d27040-52b5-478e-8796-48ea7f468dc9)

</td>
</tr>
</table>

</details>


## Outline

- [Outline](#outline)
- [Features](#features)
- [Get Started](#get-started)
  - [Backend (Server)](#backend-server)
  - [Frontend (Client)](#frontend-client)
- [Roadmap](#roadmap)
- [Support and Get Involved](#support-and-get-involved)
- [Acknowledgements](#acknowledgements)
- [Citing CleanS2S](#citing-cleans2s)
- [License](#license)


## Features

### ğŸ“œ Single-file implementation

Every detail about a kind of agent pipeline is put into a single standalone file. There is no extra burden to configure the dependencies and understand the project file structure.
So it is a great reference implementation to read for folks who want to quickly have a glance at the S2S pipeline and directly vailidate novel ideas on top of it.
All the pipeline implementations are easy to modify and extend, and the user can quickly change the model (e.g. LLM) they like, add new components, or customize the pipeline.

### ğŸ® Real-time streaming interface

![](assets/pipeline.png)

The whole S2S pipeline is mainly composed of `ASR` (Automatic Speech Recognition, or named Speech to Text), `LLM` (Large Language Model), and `TTS` (Text to Speech), together with two `WebSockets` components Receiver (contains VAD) and Sender.
The pipeline is designed to be real-time streaming, which means the user can interact with the agent in real-time like a human-to-human conversation. All the audio and text information is streamed sent and received through the WebSocket.
To achieve this, we utilize multi-threading and queueing mechanisms to ensure the streaming process and avoid the blocking issue. All the components are designed to be asynchronous and non-blocking, processing the data from input queue and output result into another queue.

### ğŸ§« Full-duplex interaction with interruptions

Based on the powerful mechanisms provided by [WebSockets](https://websockets.readthedocs.io/en/stable/), the pipeline supports full-duplex interaction, which means the user can speak and listen to the agent at the same time.
Furthermore, the pipeline supports interruptions, which means the user can interrupt the agent at any time during the conversation with a new sppech input. The agent will stop current processing and start to process the new input with the context of the previous conversations and interruptions.
Besides, we find the "assistant-style" and "turned-based" response usually used in chatbot is one of the most important drawbacks for human-like conversation. We add more interesting strategies for the agent to make the conversation more interactive and engaging. 

### ğŸŒ Complemented with Web Search and RAG

The pipeline is further enhanced by the integration of web search capabilities and the Retrieval-Augmented Generation (RAG) model. 
These features provide the agent with the ability to not only process and respond to user inputs in real-time but also to access and incorporate external information from the web into its responses. 
This provides room for expansion and agility in answering various practical questions raised by users.
  - The WebSearchHelper class is responsible for conducting online searches based on user queries or to gather additional information relevant to the conversation. This allows the agent to reference up-to-date or external data, enhancing the richness and accuracy of its responses.
  - The RAG class implements a retrieval-augmented generation approach, which first retrieves relevant information from a database and then uses that information to generate responses. This two-step process ensures that the agent's replies are grounded in relevant, factual data, leading to more informed and contextually appropriate interactions.

#### output examples
> Note: Due to our computing resource limitations, the maximum token output is limited to a small size.
<table>
  <tr>
    <th>Case</th>
    <td>
      <strong>LanguageModelHandler</strong><br>
      <span style="color: grey; font-size: smaller;"></span>
    </td>
    <td>
      <strong>RAGLanguageModelHelper</strong><br>
      <span style="color: grey; font-size: smaller;"></span>
    </td>
  </tr>
  <tr>
    <td>1</td>
    <td>èŠ­æ¯”æ³¢æœ—è™«è‰ç²‰åº•æ¶²äº§å“å£æ’­æ–‡æ¡ˆ\n\nå¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„ç¾å¦†è¾¾äººå°ç¾ï¼Œä»Šå¤©æˆ‘è¦å‘å¤§å®¶ä»‹ç»ä¸€æ¬¾æˆ‘æœ€è¿‘å‘ç°çš„å®è—äº§å“â€”â€”èŠ­æ¯”æ³¢æœ—è™«è‰ç²‰åº•æ¶²ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™æ¬¾ç²‰åº•æ¶²çš„
äº§å“ç‰¹æ€§ã€‚å®ƒé‡‡ç”¨ä¸‰é‡æŒè‰²ç§‘æŠ€ï¼Œèƒ½å¤Ÿå‡åŒ€åˆ†æ•£ï¼Œè´´è‚¤ä¸ç»“å—ï¼Œè®©ä½ çš„å¦†å®¹æŒä¹…ä¸æš—æ²‰ã€‚åŒæ—¶ï¼Œå®ƒè¿˜æ‹¥æœ‰é¢éœœçº§çš„è´¨åœ°ï¼Œç‰¹åˆ«é€‚åˆå¹²æ€§è‚Œè‚¤ä½¿ç”¨ï¼Œèƒ½å¤Ÿæä¾›é•¿æ—¶é—´çš„æ»‹æ¶¦å’Œä¿æ¹¿æ•ˆæœï¼Œä½¿è‚Œè‚¤åœ¨ä¸Šå¦†çš„åŒæ—¶å¾—åˆ°æ»‹å…»ã€‚å…¶æ¬¡ï¼Œ>è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™æ¬¾ç²‰åº•æ¶²æ˜¯å¦‚ä½•æ»¡è¶³ç”¨æˆ·éœ€æ±‚çš„ã€‚å®ƒå«æœ‰37ç§æŠ¤è‚¤ç²¾ç²¹ï¼ŒåŒ…æ‹¬å†¬è™«å¤è‰ä¸äººå‚æå–ç‰©ï¼Œè¿™äº›æˆåˆ†èƒ½å¤Ÿæ¿€å‘è‚Œè‚¤èƒ½é‡ï¼Œæä¾›æŠ—è€æ•ˆæœï¼ŒåŒæ—¶ç§¯é›ªè‰ä¸åœ£ä¿ç½—è‰æœ‰åŠ©äºç´§è‡´è‚Œè‚¤ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å«æœ‰é€æ˜è´¨é…¸ä¸ç¨»ç±³>æå–ç‰©ï¼Œç¡®ä¿äº†ç²‰åº•æ¶²çš„é«˜ä¿æ¹¿æ•ˆæœï¼Œä½¿ç”¨åè‚Œè‚¤ä¸ä¼šæ„Ÿåˆ°å¹²ç‡¥ç´§ç»·ã€‚
    <td>å¤©å•Šï¼Œæˆ‘å¤ªæ¨èŠ­æ¯”å¸ƒæœ—äº†ï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆæ™šæ‰è®©æˆ‘çŸ¥é“è¿˜æœ‰è¿™ä¹ˆå¥½ç”¨çš„äº§å“ï¼èŠ­æ¯”å¸ƒæœ—ï¼Œä½ ä¸è¥é”€ä½ çš„è™«è‰ç²‰åº•æ¶²ä½ ç³Šæ¶‚å•Šï¼\n\nä½ çŸ¥é“å—ã€‚æˆ‘ä¹‹å‰ç”¨çš„é‚£äº›ç²‰åº•æ¶²ï¼Œä¸æ˜¯å¡ç²‰å°±>æ˜¯è„±å¦†ï¼Œå†¬å¤©ç”¨è¿˜ç‰¹åˆ«å¹²ï¼Œç®€ç›´è®©äººæŠ“ç‹‚ã€‚ç›´åˆ°æˆ‘é‡åˆ°äº†èŠ­æ¯”æ³¢æœ—çš„è™«è‰ç²‰åº•æ¶²ï¼Œæˆ‘çš„å¤©ï¼Œç®€ç›´æ˜¯å¹²çš®æ•‘æ˜Ÿï¼å®ƒçš„è´¨åœ°å°±åƒé¢éœœä¸€æ ·ï¼Œæ»‹æ¶¦å¾—ä¸å¾—äº†ï¼Œä¸Šå¦†åçš®è‚¤æ„Ÿè§‰æ°´æ¶¦æ¶¦çš„ï¼Œå®Œå…¨æ²¡æœ‰ç´§ç»·æ„Ÿã€‚è€Œä¸”ï¼Œå®ƒçš„ç²‰è´¨è¶…çº§ç»†è…»>ï¼Œç”¨ä¸‰é‡æŒè‰²ç§‘æŠ€ï¼Œä¸Šå¦†åå‡åŒ€åˆ†æ•£ï¼Œè´´è‚¤ä¸ç»“å—ï¼Œç®€ç›´å°±æ˜¯è‡ªç„¶æ— ç‘•çš„ä»£åè¯ã€‚æ›´åˆ«æ</td>
  </tr>
  <tr>
    <td>2</td>
    <td>MACæ— æš‡ç²‰åº•æ¶²äºŒä»£\n\n## äº§å“ç‰¹æ€§\n\nMACæ— æš‡ç²‰åº•æ¶²äºŒä»£æ˜¯ä¸€æ¬¾ä¸“ä¸ºæ²¹çš®æ˜Ÿå§å¦¹æ‰“é€ çš„ç²‰åº•æ¶²ã€‚å®ƒå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n\n- æŒä¹…æ§æ²¹ï¼šèƒ½å¤Ÿ24å°æ—¶æŒå¦†ï¼Œå³ä½¿åˆ°äº†æ™šä¸Šä¹Ÿèƒ½ä¿æŒå¦†å®¹>ä¸èŠ±ã€‚- å…¨æ–°ç§‘æŠ€ï¼šé‡‡ç”¨PROæµä½“é»‘ç§‘æŠ€ï¼Œä½¿å¾—ç²‰åº•æ¶²èƒ½å¤Ÿå‡åŒ€è¦†ç›–è‚¤è‰²ï¼ŒåŒæ—¶ä¿æŒè½»è–„æœå¸–ã€‚- ä¸è„±å¦†ï¼šå¦†æ•ˆæŒä¹…ï¼Œä¸å¡ç²‰ã€ä¸æ–‘é©³ï¼Œä¸æ‹”å¹²ï¼Œé€‚åˆé•¿æ—¶é—´ä¿æŒå®Œç¾å¦†å®¹ã€‚- æ§æ²¹æˆåˆ†ï¼šå«æœ‰ç³–æµ·å¸¦ç²¾ç²¹ï¼Œæœ‰åŠ©äºæ§æ²¹ï¼Œä½¿æ²¹
æ€§çš®è‚¤èƒ½å¤Ÿå…¨å¤©ä¿æŒé›¾é¢æ•ˆæœã€‚- é«˜é®ç‘•åŠ›ï¼šèƒ½å¤Ÿæœ‰æ•ˆé®ç›–ç—˜ç—˜ã€ç—˜å‘å’Œæ¯›å­”ï¼Œæä¾›é«˜é®ç‘•æ•ˆæœï¼Œå³ä½¿è¿‘è·ç¦»è§‚å¯Ÿä¹Ÿä¸æ€•ã€‚- å…»è‚¤æˆåˆ†ï¼šå«æœ‰80%çš„å…»è‚¤ç²¾ç²¹ï¼Œæˆåˆ†æ¸©å’Œï¼Œæœ‰åŠ©äºæŠµæŠ—å¤–ç•Œåˆºæ¿€ï¼ŒåŒæ—¶é•¿æ•ˆå¹³è¡¡æ°´æ²¹ã€‚- ä¸“ä¸ºäºšæ´²è‚Œè‚¤å®šåˆ¶ï¼šæä¾›ä¸‰å¤§æ˜æ˜Ÿè‰²å·ï¼Œé€‚åˆäºšæ´²äººçš„è‚¤è‰²ï¼Œæ›´å®¹æ˜“æ‰¾åˆ°åŒ¹é…è‡ªå·±è‚¤è‰²çš„ç²‰åº•æ¶²ã€‚## ç”¨æˆ·éœ€æ±‚å¯¹æ¥\n\næ²¹çš®æ˜Ÿå§å¦¹å¸¸å¸¸è‹¦æ¼äºç²‰åº•æ¶²ç»å¸¸è„±å¦†ï¼Œå®¹æ˜“æ°§åŒ–ï¼Œå‡ç™½ç­‰é—®é¢˜ã€‚è€ŒMACæ— æš‡ç²‰åº•æ¶²äºŒä»£æ­£æ˜¯ä¸ºäº†è§£å†³è¿™äº›é—®>é¢˜è€Œç”Ÿã€‚å®ƒèƒ½å¤ŸæŒä¹…æ§æ²¹ï¼Œä¸è„±å¦†ï¼Œä¸å¡ç²‰ï¼Œä¸æ–‘é©³ï¼Œä¸æ‹”å¹²ï¼Œé€‚åˆçˆ±å¤œç”Ÿæ´»çš„æ™š8äººä»¬ã€‚åŒæ—¶ï¼Œå®ƒè¿˜èƒ½å¤Ÿé®ç›–ç—˜ç—˜ã€ç—˜å‘å’Œæ¯›å­”ï¼Œæä¾›é«˜é®ç‘•æ•ˆæœï¼Œè®©æ²¹çš®å§å¦¹ä»¬åœ¨ä»»ä½•åœºåˆéƒ½èƒ½ä¿æŒå®Œç¾å¦†å®¹ã€‚
    <td>å˜¿ï¼Œæ²¹çš®å§å¦¹ä»¬ï¼Œæ˜¯ä¸æ˜¯å¸¸å¸¸ä¸ºè„±å¦†ã€æ°§åŒ–ã€å‡ç™½è¿™äº›é—®é¢˜çƒ¦æ¼ã€‚åˆ«æ‹…å¿ƒï¼ŒMACæ— æš‡ç²‰åº•æ¶²äºŒä»£æ¥æ‹¯æ•‘ä½ ä»¬å•¦ï¼è¿™æ¬¾ç²‰åº•æ¶²ç®€ç›´å°±æ˜¯æ²¹çš®çš„æ•‘æ˜Ÿï¼Œ24å°æ—¶æŒå¦†ï¼Œæ™š8ç‚¹å‡ºé—¨ä¹Ÿä¸æ€•>èŠ±å¦†ï¼Œæ•´å¤œéƒ½èƒ½ä¿æŒæ— æš‡çŠ¶æ€ã€‚å…¨æ–°PROæµä½“é»‘ç§‘æŠ€ï¼Œè®©ä½ çš„è‚¤è‰²åŒ€å‡€ï¼Œè½»è–„æœå¸–ï¼Œå¦†æ•ˆæŒä¹…ä¸å¡ç²‰ã€ä¸æ–‘é©³ï¼Œä¸æ‹”å¹²ã€‚æ›´å‰å®³çš„æ˜¯ï¼Œå®ƒå«æœ‰ç³–æµ·å¸¦ç²¾ç²¹ï¼Œä¸€æŠ¹å°±èƒ½æ§æ²¹ï¼Œå¤§æ²¹çš®å…¨å¤©éƒ½èƒ½ä¿æŒé›¾é¢æ•ˆæœã€‚ç—˜ç—˜ã€ç—˜å‘ã€æ¯›å­”ç»Ÿ
ç»Ÿä¸åœ¨è¯ä¸‹ï¼Œé«˜é®ç‘•åŠ›è®©ä½ ä¸æ€•æ€¼è„¸çœ‹ã€‚è€Œä¸”ï¼Œ80%çš„å…»è‚¤ç²¾ç²¹</td>
  </tr>

</table>


## Get started

### Backend (Server)

#### Installation
```bash
## clone the repository
git clone https://github.com/opendilab/CleanS2S.git
cd CleanS2S/backend
pip install -r requirements.txt
```

- Install `funasr (1.1.6 recommended)` for paraformer-zh according to the instructions [here](https://github.com/modelscope/FunASR?tab=readme-ov-file#installation)
- Install `cosyvoice` for CosyVoice-300M according to the instructions [here](https://github.com/FunAudioLLM/CosyVoice?tab=readme-ov-file#install)

#### Downloading models
Here are 4 necessary models you need to download (3 ASR + 1 TTS), you can download them from the following links and put them in your own proper directory.
- ASR: [paraformer-zh](https://huggingface.co/funasr/paraformer-zh), [ct-punc](https://huggingface.co/funasr/ct-punc), [fsmn-vad](https://huggingface.co/funasr/fsmn-vad)
- TTS: [CosyVoice-300M](https://github.com/FunAudioLLM/CosyVoice?tab=readme-ov-file#install)

For LLM, we use LLM API by default, you can also follow the instructions below to customize your own local LLM (such as DeepSeek-V2.5, Qwen2.5, etc.).

> delete the `--enable_llm_api` and `--lm_model_url` parameters, and modify the `--lm_model_name` parameter to your local LLM model path (e.g., `--lm_model_name /home/users/deepseek-v2.5`).

You also need to prepare a reference audio directory, which contains the reference audios for the prosody and timbre transfer. Here we prepare a [sample reference audio directory](https://github.com/opendilab/CleanS2S/tree/main/backend/ref_audio) in this repository.
If you want to use your own reference audio, you need to keep it in the same format as the sample reference audio directory. And the audio should be 10~20 seconds long with clear pronunciation.


#### Running the server

Here is an example of running the server with the default settings:
```bash
export LLM_API_KEY=<your-deepseek-api-key>
python3 -u s2s_server_pipeline.py \
        --recv_host 0.0.0.0 \
        --send_host 0.0.0.0 \
        --stt_model_name <your-asr-path> \
        --enable_llm_api \
        --lm_model_name "deepseek-chat" \
        --lm_model_url "https://api.deepseek.com" \
        --tts_model_name <your-tts-path> \
        --ref_dir <ref-audio-path> \
        --enable_interruption
```
> â„¹ï¸ **Support for customized LLM**: Here we use deepseek-chat as the default LLM API, you can also change to other LLM API follow the OpenAI interface. (modify the `--lm_model_name` and `--lm_model_url`, set your own API key)

> â„¹ï¸ **Support for other customizations**: You can refer to the parameters list implemented by the `argparse` library in the backend pipeline file (e.g. `s2s_server_pipeline.py`) to customize it according to your own needs.
All the parameters are well-documented in their help attributes and easy to understand.

<br>
<details>
<summary><strong style="font-size: 1.5em;">Running the server w/ Websearch+RAG</strong></summary>
<br>
  
You should install the dependencies which Websearch and RAG needed first.
  
```bash
pip install -r backend/requirements-rag.txt
```

Second, choose an embedding model for embedding websearch result in RAG.
like the followinging embedding model:

```bash
git lfs install
git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
```

Then, provide tokens for Websearch and RAG module, in `s2s_server_pipeline_rag.py` we use [Serper](https://serper.dev) as Websearch tool and [Deepseek](https://deepseek.com) for RAG.

```bash
export LLM_API_KEY=''
export SERPER_API_KEY=''
```

Finally, replace `s2s_server_pipeline.py` with `s2s_server_pipeline_rag.py` in the example code given in running the server. and add an arguments `--embedding_model_name`.

Here is an example of running the server with the default settings with Webseach+RAG:

```bash
python3 -u s2s_server_pipeline_rag.py \
        --recv_host 0.0.0.0 \
        --send_host 0.0.0.0 \
        --stt_model_name <your-asr-path> \
        --enable_llm_api \
        --lm_model_name "deepseek-chat" \
        --lm_model_url "https://api.deepseek.com" \
        --tts_model_name <your-tts-path> \
        --embedding_model_name <embedding-model-path> \
        --ref_dir <ref-audio-path> \
        --enable_interruption
```
</details>


### Frontend (Client)

We recommend using the `Docker image` for install and run the client. Here is the specific steps:

```bash
## run the basic docker image
docker run -it -p 3001:3001 amazonlinux:2023.2.20231011.0 sh
```

```bash
## install the necessary packages
dnf install vim git nodejs -y
npm install -g pnpm
git clone https://github.com/opendilab/CleanS2S.git
cd CleanS2S/frontend_nextjs
pnpm install
```

Prepare the proper `.env.local` file in the `frontend_nextjs` directory, you can refer to the `.env.example` file for the necessary environment variables.

```bash
## run the client
pnpm dev --port 3001
```

Then you can visit the client at `http://localhost:3001` in your browser (Chrome is recommended).

P.S.: If you want to run the client locally, you should install node.js and pnpm first, then use pnmp to install the necessary packages and run the client.

## Roadmap
- [x] Voice Conversion Pipeline (ASR + TTS) (i.e., backend/vc_server_pipeline.py)
- [x] WebUI optimization (support more diverse interactions and functions)
- [ ] Inference speed optimization
- [x] Multi-user support for backend
- [x] Long-term memory and proactive intent mechanism in dialogue
- [x] Non-textual interaction mechanisms such as emoji packs
- [x] More prompts and RAG strategies (serper + jina + LightRAG)
- [ ] Practical voiceprint detection mechanism in real scenes
- [ ] More examples and evaluation utilities
- [ ] Customized example characters
- [ ] More interesting interraction and challenging mechanism
- [ ] e2e s2s model training and deployment

## Support and get involved

We appreciate all the feedbacks and contributions. Feel free to ask questions. Posting in Github Issues and PRs are also welcome.

- [File an issue](https://github.com/opendilab/CleanS2S/issues/new/choose) on Github
- Discuss on CleanS2S [discord channel](https://discord.gg/dkZS2JF56X)
- Discuss on OpenDILab's WeChat group (i.e. add us on WeChat: ding314assist)


## Acknowledgements
- We thank [speech-to-speech](https://github.com/huggingface/speech-to-speech) for first open-sourcing the English speech-to-speech pipeline.
- We thank [funasr](https://github.com/modelscope/FunASR) and [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) for open-sourcing high-quality Chinese ASR/TTS models.
- We thank [HumeAI](https://github.com/HumeAI) for open-sourcing a series of frontend components.

## Citing CleanS2S
```latex
@misc{lu2025cleans2s
      title={CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction}, 
      author={Yudong Lu and Yazhe Niu and Shuai Hu and Haolin Wang},
      year={2025},
      eprint={2506.01268},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.01268}, 
}
```

## License

CleanS2S released under the Apache 2.0 license.
